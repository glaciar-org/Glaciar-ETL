input {  

  file {
    type => "dataset_01_2009_2017_BO"
    path => "${ELK_STACK_UELA_DATASET}/DATASET_01_2009_2017_BO.csv"
    # path => "${ELK_STACK_UELA_DATASET}/DATASET_01_2009_2017_BO.small.borrar.csv"
    start_position => "beginning"
    sincedb_path => "/dev/null" 
  }
  
}
filter {  

    # FECHA,HORA,AIRQ_CO,AIRQ_NO2,DATASET
    # 01/10/2009,14, , ,DATASET_01_2009_2017_BO
    # 01/10/2009,15,0.16,25,DATASET_01_2009_2017_BO
    # 01/10/2009,16,0.14,23,DATASET_01_2009_2017_BO
    # 01/10/2009,17,0.12,24,DATASET_01_2009_2017_BO

  csv {
      separator => ","
      columns => ["DATASET","FECHA","HORA","AIRQ_CO","AIRQ_NO2"]
  }
  

  mutate {
    add_field => { "fecha_time_stamp" => "%{FECHA} %{HORA}" }
  }

  date {

    #Â If you want to have "logtimestamp" become a time object,
    #    you need to add target => "logtimestamp" to your date filter block.
    # By default, the date filter overwrites the @timestamp field with the value of the matched field,
    #    in this case, logtimestamp's value. 
    # match => [ "FECHA", "dd/MM/yyyy" ]

    # match => [ "fecha_time_stamp", "dd/MM/yyyy HH:mm:ss" ]
    #    match => [ "HORA", "HH:mm:ss" ]
    # match => [ "fecha_time_stamp", "dd/MM/yyyy" ]
    match => [ "fecha_time_stamp", "dd/MM/yyyy HH" ]
    target => "FECHA_HORA"
    timezone => "Etc/GMT"
  }


  alter {
    condrewrite => [
         "AIRQ_CO", " ", "0",
         "AIRQ_NO", " ", "0",
         "AIRQ_NO2", " ", "0"
       ]
  }

 #mutate {convert => ["FECHA",  "float"]}
  mutate {convert => ["AIRQ_CO",  "float"]}
  mutate {convert => ["AIRQ_NO",  "float"]}
  mutate {convert => ["AIRQ_NO2",  "float"]}


}
output {  

    # elasticsearch {
    #     hosts => "http://localhost:9200"
    #     index => "uela-dataset-01-2010"
    # }

    elasticsearch {
        index    => "uela-dataset-01-small-bo5"
        hosts    => "${ELASTIC_xHOST}"
        user     => "${ELASTIC_xUSER}"
        password => "${ELASTIC_xPASS}"
    }   
  
  
    stdout {}

}

