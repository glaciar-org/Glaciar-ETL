input {  

  file {
    type => "dataset_01_2009_2017_BO"
    path => "${ELK_STACK_UELA_DATASET}/DATASET_01_2009_2017_BO.csv"
    start_position => "beginning"
    sincedb_path => "/dev/null" 
  }
  file {
    type => "dataset_01_2009_2017_CE"
    path => "${ELK_STACK_UELA_DATASET}/DATASET_01_2009_2017_CE.csv"
    start_position => "beginning"
    sincedb_path => "/dev/null" 
  }
  file {
    type => "dataset_01_2009_2017_CO"
    path => "${ELK_STACK_UELA_DATASET}/DATASET_01_2009_2017_CO.csv"
    start_position => "beginning"
    sincedb_path => "/dev/null" 
  }
  file {
    type => "dataset_01_2009_2017_PA"
    path => "${ELK_STACK_UELA_DATASET}/DATASET_01_2009_2017_PA.csv"
    start_position => "beginning"
    sincedb_path => "/dev/null" 
  }
}
filter {  

    # FECHA,HORA,AIRQ_CO,AIRQ_NO2,DATASET
    # 01/10/2009,14, , ,DATASET_01_2009_2017_BO
    # 01/10/2009,15,0.16,25,DATASET_01_2009_2017_BO
    # 01/10/2009,16,0.14,23,DATASET_01_2009_2017_BO
    # 01/10/2009,17,0.12,24,DATASET_01_2009_2017_BO

  csv {
      separator => ","
      columns => ["DATASET","FECHA","HORA","AIRQ_CO","AIRQ_NO2"]
  }
  

  mutate {
    add_field => { "fecha_time_stamp" => "%{FECHA} %{HORA}" }
  }

  date {

    #Â If you want to have "logtimestamp" become a time object,
    #    you need to add target => "logtimestamp" to your date filter block.
    # By default, the date filter overwrites the @timestamp field with the value of the matched field,
    #    in this case, logtimestamp's value. 
    # match => [ "FECHA", "dd/MM/yyyy" ]

    match => [ "fecha_time_stamp", "dd/MM/yyyy HH" ]
    target => "FECHA_HORA"
    timezone => "Etc/GMT"
  }


  alter {
    condrewrite => [
         "AIRQ_CO", " ", "0",
         "AIRQ_NO", " ", "0",
         "AIRQ_NO2", " ", "0"
       ]
  }

 #mutate {convert => ["FECHA",  "float"]}
  mutate {convert => ["AIRQ_CO",  "float"]}
  mutate {convert => ["AIRQ_NO",  "float"]}
  mutate {convert => ["AIRQ_NO2",  "float"]}


  # Como construir un GEO POINT CON KIBANA (Creo que es el mas claro ...)
  # https://discuss.elastic.co/t/how-to-construct-geo-point-field-from-separate-fields-of-latitude-and-longitude-for-kibana-5-4/91755

  # NOTA: PARA QUE ANDE ESTO, tengo que hacer un post primero ... 

  # BS AS DATA:
  # https://data.buenosaires.gob.ar/dataset/calidad-de-aire
  # long              lat              	DIRECCION	  INICIO_DE_ACTIVIDAD	PARAMETROS_MEDIDOS	
  # -58,3663735070165	-34,6252584813295	LA BOCA	AV. 01/05/2009        	CO, NO, NO2, NOX, PM10
  # -58,4320717652753	-34,6066080998154	CENTENARIO	01/01/2005          CO, NO, NO2, NOX, PM10
  # -58,391552893462	-34,5995643433432	CORDOBA	AV. 01/05/2009          CO, NO, NO2, NOX, PM10
  # -58,4053598727298	-34,5834529467442	PALERMO	AV. 01/01/2002          CO, NO, NO2, NOX, PM10

  if [type] == "dataset_01_2009_2017_BO" {
      mutate {
        add_field => { "[location][lat]" => "-34.6252584813295" }
        add_field => { "[location][lon]" => "-58.3663735070165" }
      }
  } else if [type] == "dataset_01_2009_2017_CE" {
      mutate {
        add_field => { "[location][lat]" => "-34.6066080998154" }
        add_field => { "[location][lon]" => "-58.4320717652753" }
      }
  } else if [type] == "dataset_01_2009_2017_CO" {
      mutate {
        add_field => { "[location][lat]" => "-34.5995643433432" }
        add_field => { "[location][lon]" => "-58.391552893462"  }
      }
  } else if [type] == "dataset_01_2009_2017_PA" {
      mutate {
        add_field => { "[location][lat]" => "-34.5834529467442" }
        add_field => { "[location][lon]" => "-58.4053598727298" }
      }
  }
  mutate {
    convert => {"[location][lat]" => "float"}
    convert => {"[location][lon]" => "float"}
  }


}
output {  

    # elasticsearch {
    #     hosts => "http://localhost:9200"
    #     index => "uela-dataset-01-all"
    # }

    elasticsearch {
        index    => "uela-dataset-01-geo-all-v2"
        hosts    => "${ELASTIC_xHOST}"
        user     => "${ELASTIC_xUSER}"
        password => "${ELASTIC_xPASS}"
    }   

    

    stdout {}

}

